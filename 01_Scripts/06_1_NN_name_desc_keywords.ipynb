{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:13.593110Z","iopub.status.busy":"2023-06-26T15:19:13.592759Z","iopub.status.idle":"2023-06-26T15:19:28.011135Z","shell.execute_reply":"2023-06-26T15:19:28.009725Z","shell.execute_reply.started":"2023-06-26T15:19:13.593079Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]},{"data":{"text/plain":["device(type='cpu')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","import inflect\n","from tqdm.notebook import tqdm\n","from nltk.corpus import stopwords\n","STOPWORDS = stopwords.words('english')\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:28.016729Z","iopub.status.busy":"2023-06-26T15:19:28.015908Z","iopub.status.idle":"2023-06-26T15:19:28.024340Z","shell.execute_reply":"2023-06-26T15:19:28.023168Z","shell.execute_reply.started":"2023-06-26T15:19:28.016690Z"},"trusted":true},"outputs":[],"source":["COLS_TO_DROP = ['PROJECT_ID','BACKERS_COUNT']\n","TEXT_FEATURES = ['NAME', 'DESC', 'KEYWORDS']\n","DATE_FEATURES = ['DEADLINE', 'STATE_CHANGED_AT', 'CREATED_AT', 'LAUNCHED_AT']\n","NOMINAL_FEATURES = ['DISABLE_COMMUNICATION', 'COUNTRY', 'CURRENCY']\n","TO_SCALE = ['GOAL', 'CREATE_LAUNCH_HOURS', 'CREATE_LAUNCH_HOURS_LOG', 'CREATE_DEADLINE_HOURS', 'CREATE_DEADLINE_HOURS_LOG', 'LAUNCHED_DEADLINE_HOURS', 'month', 'day', 'hour', 'minute', 'second']"]},{"cell_type":"markdown","metadata":{},"source":["# Load data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:28.026471Z","iopub.status.busy":"2023-06-26T15:19:28.025985Z","iopub.status.idle":"2023-06-26T15:19:28.686490Z","shell.execute_reply":"2023-06-26T15:19:28.684917Z","shell.execute_reply.started":"2023-06-26T15:19:28.026438Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PROJECT_ID</th>\n","      <th>NAME</th>\n","      <th>DESC</th>\n","      <th>GOAL</th>\n","      <th>KEYWORDS</th>\n","      <th>DISABLE_COMMUNICATION</th>\n","      <th>COUNTRY</th>\n","      <th>CURRENCY</th>\n","      <th>DEADLINE</th>\n","      <th>STATE_CHANGED_AT</th>\n","      <th>CREATED_AT</th>\n","      <th>LAUNCHED_AT</th>\n","      <th>BACKERS_COUNT</th>\n","      <th>FINAL_STATUS</th>\n","      <th>CREATE_LAUNCH_HOURS</th>\n","      <th>CREATE_LAUNCH_HOURS_LOG</th>\n","      <th>CREATE_DEADLINE_HOURS</th>\n","      <th>CREATE_DEADLINE_HOURS_LOG</th>\n","      <th>LAUNCHED_DEADLINE_HOURS</th>\n","      <th>TRAIN_VAL_TEST_SPLIT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kkst124894863</td>\n","      <td>Mall Of The Internet</td>\n","      <td>First Virtual Reality Mall, eCommerce, Live Au...</td>\n","      <td>60000.0</td>\n","      <td>mall-of-the-internet</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2015-04-22 18:21:56</td>\n","      <td>2015-04-22 18:21:56</td>\n","      <td>2014-12-14 18:32:03</td>\n","      <td>2015-03-23 18:21:56</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2375.831389</td>\n","      <td>7.773524</td>\n","      <td>3095.831389</td>\n","      <td>8.038135</td>\n","      <td>720.000000</td>\n","      <td>Train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>kkst456144988</td>\n","      <td>The Don't Tell Darlings' New Album: \"\"\"\"\"\"\"\"\"\"...</td>\n","      <td>\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...</td>\n","      <td>800.0</td>\n","      <td>the-dont-tell-darlings-new-album-sugar-for-sugar</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2011-09-23 04:00:00</td>\n","      <td>2011-09-23 04:00:53</td>\n","      <td>2011-07-26 19:20:33</td>\n","      <td>2011-07-31 18:47:42</td>\n","      <td>41</td>\n","      <td>1</td>\n","      <td>119.452500</td>\n","      <td>4.791255</td>\n","      <td>1400.657500</td>\n","      <td>7.245411</td>\n","      <td>1281.205000</td>\n","      <td>Train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kkst598563858</td>\n","      <td>OPEN SOURCE GAME Busters Nuts!</td>\n","      <td>Big Buck Bunny mobile game is a open source AP...</td>\n","      <td>10000.0</td>\n","      <td>busters-nuts</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2013-06-20 03:19:57</td>\n","      <td>2013-06-20 03:19:57</td>\n","      <td>2012-08-24 17:51:43</td>\n","      <td>2013-05-16 03:19:57</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6345.470556</td>\n","      <td>8.755654</td>\n","      <td>7185.470556</td>\n","      <td>8.879955</td>\n","      <td>840.000000</td>\n","      <td>Train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>kkst1129209536</td>\n","      <td>Send SueNami - DRAG Olympic</td>\n","      <td>Olympics for Drag Queens? From Dress to Succes...</td>\n","      <td>270.0</td>\n","      <td>send-suenami-drag-olympic</td>\n","      <td>False</td>\n","      <td>GB</td>\n","      <td>GBP</td>\n","      <td>2014-07-18 22:20:18</td>\n","      <td>2014-07-18 22:20:18</td>\n","      <td>2014-07-10 21:25:03</td>\n","      <td>2014-07-10 22:20:18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.920833</td>\n","      <td>0.652759</td>\n","      <td>192.920833</td>\n","      <td>5.267450</td>\n","      <td>192.000000</td>\n","      <td>Train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kkst8900047</td>\n","      <td>Knowing Nile S</td>\n","      <td>\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...</td>\n","      <td>5.0</td>\n","      <td>knowing-nile-s</td>\n","      <td>False</td>\n","      <td>GB</td>\n","      <td>GBP</td>\n","      <td>2013-11-10 16:51:02</td>\n","      <td>2013-11-10 16:51:02</td>\n","      <td>2013-09-07 08:37:01</td>\n","      <td>2013-10-11 15:51:02</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>823.233611</td>\n","      <td>6.714454</td>\n","      <td>1544.233611</td>\n","      <td>7.342930</td>\n","      <td>721.000000</td>\n","      <td>Train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>108124</th>\n","      <td>kkst1377169294</td>\n","      <td>Somewhere Out There</td>\n","      <td>Creating a smile, not just any smile, the smil...</td>\n","      <td>250.0</td>\n","      <td>somewhere-out-there</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2014-09-14 03:00:00</td>\n","      <td>2014-09-14 03:00:10</td>\n","      <td>2014-07-18 21:49:37</td>\n","      <td>2014-08-22 16:55:17</td>\n","      <td>24</td>\n","      <td>1</td>\n","      <td>835.094444</td>\n","      <td>6.728742</td>\n","      <td>1373.173056</td>\n","      <td>7.225607</td>\n","      <td>538.078611</td>\n","      <td>Test</td>\n","    </tr>\n","    <tr>\n","      <th>108125</th>\n","      <td>kkst450010306</td>\n","      <td>Time Tracker Android Application</td>\n","      <td>Time tracker is an application that calculates...</td>\n","      <td>5000.0</td>\n","      <td>time-tracker-android-application</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2013-05-02 18:39:22</td>\n","      <td>2013-05-02 18:39:25</td>\n","      <td>2013-03-21 20:50:18</td>\n","      <td>2013-04-02 18:39:22</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>285.817778</td>\n","      <td>5.658847</td>\n","      <td>1005.817778</td>\n","      <td>6.914550</td>\n","      <td>720.000000</td>\n","      <td>Test</td>\n","    </tr>\n","    <tr>\n","      <th>108126</th>\n","      <td>kkst280861137</td>\n","      <td>Living Ruins</td>\n","      <td>A public sculpture and its documentary to show...</td>\n","      <td>45000.0</td>\n","      <td>living-ruins</td>\n","      <td>False</td>\n","      <td>CA</td>\n","      <td>CAD</td>\n","      <td>2015-04-22 01:13:36</td>\n","      <td>2015-04-22 01:13:38</td>\n","      <td>2015-03-10 02:17:17</td>\n","      <td>2015-03-23 01:13:36</td>\n","      <td>36</td>\n","      <td>0</td>\n","      <td>310.938611</td>\n","      <td>5.742806</td>\n","      <td>1030.938611</td>\n","      <td>6.939194</td>\n","      <td>720.000000</td>\n","      <td>Test</td>\n","    </tr>\n","    <tr>\n","      <th>108127</th>\n","      <td>kkst1638562722</td>\n","      <td>iCulture Collection - fine poster prints, stic...</td>\n","      <td>iCulture project is a comprehensive detailed c...</td>\n","      <td>3000.0</td>\n","      <td>iculture-collection-fine-poster-prints-sticker...</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2013-11-21 19:09:13</td>\n","      <td>2013-11-12 00:39:08</td>\n","      <td>2013-10-16 09:31:06</td>\n","      <td>2013-10-22 18:09:13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>152.635278</td>\n","      <td>5.034581</td>\n","      <td>873.635278</td>\n","      <td>6.773807</td>\n","      <td>721.000000</td>\n","      <td>Test</td>\n","    </tr>\n","    <tr>\n","      <th>108128</th>\n","      <td>kkst1174254808</td>\n","      <td>Fist Full of Zombies the Movie</td>\n","      <td>We need your help to raise funds for the post ...</td>\n","      <td>400.0</td>\n","      <td>fist-full-of-zombies-the-movie</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>2011-08-30 17:33:24</td>\n","      <td>2011-08-30 17:33:26</td>\n","      <td>2011-07-16 16:58:04</td>\n","      <td>2011-07-16 17:33:24</td>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>0.588889</td>\n","      <td>0.463035</td>\n","      <td>1080.588889</td>\n","      <td>6.986186</td>\n","      <td>1080.000000</td>\n","      <td>Test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>108129 rows Ã— 20 columns</p>\n","</div>"],"text/plain":["            PROJECT_ID                                               NAME  \\\n","0        kkst124894863                               Mall Of The Internet   \n","1        kkst456144988  The Don't Tell Darlings' New Album: \"\"\"\"\"\"\"\"\"\"...   \n","2        kkst598563858                     OPEN SOURCE GAME Busters Nuts!   \n","3       kkst1129209536                        Send SueNami - DRAG Olympic   \n","4          kkst8900047                                     Knowing Nile S   \n","...                ...                                                ...   \n","108124  kkst1377169294                                Somewhere Out There   \n","108125   kkst450010306                   Time Tracker Android Application   \n","108126   kkst280861137                                       Living Ruins   \n","108127  kkst1638562722  iCulture Collection - fine poster prints, stic...   \n","108128  kkst1174254808                     Fist Full of Zombies the Movie   \n","\n","                                                     DESC     GOAL  \\\n","0       First Virtual Reality Mall, eCommerce, Live Au...  60000.0   \n","1       \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...    800.0   \n","2       Big Buck Bunny mobile game is a open source AP...  10000.0   \n","3       Olympics for Drag Queens? From Dress to Succes...    270.0   \n","4       \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...      5.0   \n","...                                                   ...      ...   \n","108124  Creating a smile, not just any smile, the smil...    250.0   \n","108125  Time tracker is an application that calculates...   5000.0   \n","108126  A public sculpture and its documentary to show...  45000.0   \n","108127  iCulture project is a comprehensive detailed c...   3000.0   \n","108128  We need your help to raise funds for the post ...    400.0   \n","\n","                                                 KEYWORDS  \\\n","0                                    mall-of-the-internet   \n","1        the-dont-tell-darlings-new-album-sugar-for-sugar   \n","2                                            busters-nuts   \n","3                               send-suenami-drag-olympic   \n","4                                          knowing-nile-s   \n","...                                                   ...   \n","108124                                somewhere-out-there   \n","108125                   time-tracker-android-application   \n","108126                                       living-ruins   \n","108127  iculture-collection-fine-poster-prints-sticker...   \n","108128                     fist-full-of-zombies-the-movie   \n","\n","        DISABLE_COMMUNICATION COUNTRY CURRENCY            DEADLINE  \\\n","0                       False      US      USD 2015-04-22 18:21:56   \n","1                       False      US      USD 2011-09-23 04:00:00   \n","2                       False      US      USD 2013-06-20 03:19:57   \n","3                       False      GB      GBP 2014-07-18 22:20:18   \n","4                       False      GB      GBP 2013-11-10 16:51:02   \n","...                       ...     ...      ...                 ...   \n","108124                  False      US      USD 2014-09-14 03:00:00   \n","108125                  False      US      USD 2013-05-02 18:39:22   \n","108126                  False      CA      CAD 2015-04-22 01:13:36   \n","108127                  False      US      USD 2013-11-21 19:09:13   \n","108128                  False      US      USD 2011-08-30 17:33:24   \n","\n","          STATE_CHANGED_AT          CREATED_AT         LAUNCHED_AT  \\\n","0      2015-04-22 18:21:56 2014-12-14 18:32:03 2015-03-23 18:21:56   \n","1      2011-09-23 04:00:53 2011-07-26 19:20:33 2011-07-31 18:47:42   \n","2      2013-06-20 03:19:57 2012-08-24 17:51:43 2013-05-16 03:19:57   \n","3      2014-07-18 22:20:18 2014-07-10 21:25:03 2014-07-10 22:20:18   \n","4      2013-11-10 16:51:02 2013-09-07 08:37:01 2013-10-11 15:51:02   \n","...                    ...                 ...                 ...   \n","108124 2014-09-14 03:00:10 2014-07-18 21:49:37 2014-08-22 16:55:17   \n","108125 2013-05-02 18:39:25 2013-03-21 20:50:18 2013-04-02 18:39:22   \n","108126 2015-04-22 01:13:38 2015-03-10 02:17:17 2015-03-23 01:13:36   \n","108127 2013-11-12 00:39:08 2013-10-16 09:31:06 2013-10-22 18:09:13   \n","108128 2011-08-30 17:33:26 2011-07-16 16:58:04 2011-07-16 17:33:24   \n","\n","        BACKERS_COUNT  FINAL_STATUS  CREATE_LAUNCH_HOURS  \\\n","0                   4             0          2375.831389   \n","1                  41             1           119.452500   \n","2                   1             0          6345.470556   \n","3                   0             0             0.920833   \n","4                   0             0           823.233611   \n","...               ...           ...                  ...   \n","108124             24             1           835.094444   \n","108125              0             0           285.817778   \n","108126             36             0           310.938611   \n","108127              0             0           152.635278   \n","108128             14             1             0.588889   \n","\n","        CREATE_LAUNCH_HOURS_LOG  CREATE_DEADLINE_HOURS  \\\n","0                      7.773524            3095.831389   \n","1                      4.791255            1400.657500   \n","2                      8.755654            7185.470556   \n","3                      0.652759             192.920833   \n","4                      6.714454            1544.233611   \n","...                         ...                    ...   \n","108124                 6.728742            1373.173056   \n","108125                 5.658847            1005.817778   \n","108126                 5.742806            1030.938611   \n","108127                 5.034581             873.635278   \n","108128                 0.463035            1080.588889   \n","\n","        CREATE_DEADLINE_HOURS_LOG  LAUNCHED_DEADLINE_HOURS  \\\n","0                        8.038135               720.000000   \n","1                        7.245411              1281.205000   \n","2                        8.879955               840.000000   \n","3                        5.267450               192.000000   \n","4                        7.342930               721.000000   \n","...                           ...                      ...   \n","108124                   7.225607               538.078611   \n","108125                   6.914550               720.000000   \n","108126                   6.939194               720.000000   \n","108127                   6.773807               721.000000   \n","108128                   6.986186              1080.000000   \n","\n","       TRAIN_VAL_TEST_SPLIT  \n","0                     Train  \n","1                     Train  \n","2                     Train  \n","3                     Train  \n","4                     Train  \n","...                     ...  \n","108124                 Test  \n","108125                 Test  \n","108126                 Test  \n","108127                 Test  \n","108128                 Test  \n","\n","[108129 rows x 20 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["filepath = fr'/kaggle/input/kickstarter/01_df_development.pkl'\n","df_development = pd.read_pickle(filepath)\n","df_development"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:28.689604Z","iopub.status.busy":"2023-06-26T15:19:28.687673Z","iopub.status.idle":"2023-06-26T15:19:28.971347Z","shell.execute_reply":"2023-06-26T15:19:28.970051Z","shell.execute_reply.started":"2023-06-26T15:19:28.689571Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 108129 entries, 0 to 108128\n","Data columns (total 20 columns):\n"," #   Column                     Non-Null Count   Dtype         \n","---  ------                     --------------   -----         \n"," 0   PROJECT_ID                 108129 non-null  object        \n"," 1   NAME                       108129 non-null  object        \n"," 2   DESC                       108129 non-null  object        \n"," 3   GOAL                       108129 non-null  float64       \n"," 4   KEYWORDS                   108129 non-null  object        \n"," 5   DISABLE_COMMUNICATION      108129 non-null  bool          \n"," 6   COUNTRY                    108129 non-null  object        \n"," 7   CURRENCY                   108129 non-null  object        \n"," 8   DEADLINE                   108129 non-null  datetime64[ns]\n"," 9   STATE_CHANGED_AT           108129 non-null  datetime64[ns]\n"," 10  CREATED_AT                 108129 non-null  datetime64[ns]\n"," 11  LAUNCHED_AT                108129 non-null  datetime64[ns]\n"," 12  BACKERS_COUNT              108129 non-null  int64         \n"," 13  FINAL_STATUS               108129 non-null  int64         \n"," 14  CREATE_LAUNCH_HOURS        108129 non-null  float64       \n"," 15  CREATE_LAUNCH_HOURS_LOG    108129 non-null  float64       \n"," 16  CREATE_DEADLINE_HOURS      108129 non-null  float64       \n"," 17  CREATE_DEADLINE_HOURS_LOG  108129 non-null  float64       \n"," 18  LAUNCHED_DEADLINE_HOURS    108129 non-null  float64       \n"," 19  TRAIN_VAL_TEST_SPLIT       108129 non-null  object        \n","dtypes: bool(1), datetime64[ns](4), float64(6), int64(2), object(7)\n","memory usage: 15.8+ MB\n"]}],"source":["df_development.info()"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:28.973194Z","iopub.status.busy":"2023-06-26T15:19:28.972855Z","iopub.status.idle":"2023-06-26T15:19:28.981805Z","shell.execute_reply":"2023-06-26T15:19:28.980720Z","shell.execute_reply.started":"2023-06-26T15:19:28.973165Z"},"trusted":true},"outputs":[],"source":["p = inflect.engine()\n","\n","def convert_numbers_to_words(text):\n","    words = []\n","    for word in text.split():\n","        if word.isdigit():\n","            try:\n","                word = p.number_to_words(int(word))\n","            except Exception as e:\n","                continue\n","        else:\n","            # Use regular expression to separate numbers and non-numbers\n","            parts = re.findall(r'\\d+|\\D+', word)\n","            parts = [p.number_to_words(int(part)) if part.isdigit() else part for part in parts]\n","            word = ' '.join(parts)\n","        words.append(word)\n","    return ' '.join(words)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:28.984034Z","iopub.status.busy":"2023-06-26T15:19:28.983428Z","iopub.status.idle":"2023-06-26T15:19:29.001304Z","shell.execute_reply":"2023-06-26T15:19:28.999801Z","shell.execute_reply.started":"2023-06-26T15:19:28.984001Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'twenty-seven mpg'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["convert_numbers_to_words('27mpg')"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:19:29.003777Z","iopub.status.busy":"2023-06-26T15:19:29.003359Z","iopub.status.idle":"2023-06-26T15:19:29.039546Z","shell.execute_reply":"2023-06-26T15:19:29.038406Z","shell.execute_reply.started":"2023-06-26T15:19:29.003743Z"},"trusted":true},"outputs":[],"source":["def process_texts(texts):\n","    texts = texts.copy()\n","    texts = texts.str.replace('\"', '')\n","    texts = texts.str.replace(\"'\", '')\n","    texts = texts.apply(convert_numbers_to_words)\n","    texts = texts.apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in STOPWORDS]))\n","    return texts\n","\n","\n","def get_sequences(texts, max_seq_length=None):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(texts)\n","    print(f'Vocab length: {len(tokenizer.word_index) + 1}')\n","    \n","    sequences = tokenizer.texts_to_sequences(texts)\n","    \n","    if max_seq_length is None:\n","        max_seq_length = np.max(list(map(len, sequences)))\n","    sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n","    \n","    return sequences, max_seq_length\n","\n","\n","def onehot_encode(df, column):\n","    df = df.copy()\n","    one_hots = pd.get_dummies(df[column], prefix=column, drop_first=True)\n","    df = pd.concat([df, one_hots], axis=1)\n","    return df\n","\n","\n","def encode_dates(df, column):\n","    df = df.copy()\n","    col_dt = pd.to_datetime(df[column])\n","    df['month'] = col_dt.dt.month\n","    df['day'] = col_dt.dt.day\n","    df['hour'] = col_dt.dt.hour\n","    df['minute'] = col_dt.dt.minute\n","    df['second'] = col_dt.dt.second\n","    return df\n","\n","\n","def preprocess_inputs(df):\n","    df = df.copy()\n","    \n","    df = df.drop(COLS_TO_DROP, axis=1)\n","    \n","    df['KEYWORDS'] = df['KEYWORDS'].str.replace('-', ' ').str.strip()\n","    \n","    for text_feature in TEXT_FEATURES:\n","        df[text_feature] = process_texts(df[text_feature])\n","    \n","    for date_feature in DATE_FEATURES:\n","        df = encode_dates(df, date_feature)\n","        df = df.drop(date_feature, axis=1)\n","        \n","    for nominal_feature in NOMINAL_FEATURES:\n","        df = onehot_encode(df, nominal_feature)\n","        df = df.drop(nominal_feature, axis=1)\n","    \n","    df_train = df.query('TRAIN_VAL_TEST_SPLIT == \"Train\"')\n","    df_val = df.query('TRAIN_VAL_TEST_SPLIT == \"Validation\"')\n","    df_test = df.query('TRAIN_VAL_TEST_SPLIT == \"Test\"')\n","    \n","    y_train = df_train['FINAL_STATUS']\n","    y_val = df_val['FINAL_STATUS']\n","    y_test = df_test['FINAL_STATUS']\n","    \n","    X_train = df_train.drop('FINAL_STATUS', axis=1)\n","    X_val = df_val.drop('FINAL_STATUS', axis=1)\n","    X_test = df_test.drop('FINAL_STATUS', axis=1)\n","    \n","    print('Names')\n","    names_train, max_seq_length = get_sequences(X_train['NAME'])\n","    names_val, _ = get_sequences(X_val['NAME'], max_seq_length)\n","    names_test, _ = get_sequences(X_test['NAME'], max_seq_length)\n","    \n","    print('Descriptions')\n","    desc_train, max_seq_length = get_sequences(X_train['DESC'])\n","    desc_val, _ = get_sequences(X_val['DESC'], max_seq_length)\n","    desc_test, _ = get_sequences(X_test['DESC'], max_seq_length)\n","    \n","    print('Keywords')\n","    keywords_train, max_seq_length = get_sequences(X_train['KEYWORDS'])\n","    keywords_val, _ = get_sequences(X_val['KEYWORDS'], max_seq_length)\n","    keywords_test, _ = get_sequences(X_test['KEYWORDS'], max_seq_length)\n","    \n","    X_train = X_train.drop(TEXT_FEATURES + ['TRAIN_VAL_TEST_SPLIT'], axis=1)\n","    X_val = X_val.drop(TEXT_FEATURES + ['TRAIN_VAL_TEST_SPLIT'], axis=1)\n","    X_test = X_test.drop(TEXT_FEATURES + ['TRAIN_VAL_TEST_SPLIT'], axis=1)\n","    \n","    scaler = StandardScaler()\n","    scaler.fit(X_train[TO_SCALE])\n","    X_train[TO_SCALE] = pd.DataFrame(scaler.transform(X_train[TO_SCALE]), index=X_train.index, columns=TO_SCALE)\n","    X_val[TO_SCALE] = pd.DataFrame(scaler.transform(X_val[TO_SCALE]), index=X_val.index, columns=TO_SCALE)\n","    X_test[TO_SCALE] = pd.DataFrame(scaler.transform(X_test[TO_SCALE]), index=X_test.index, columns=TO_SCALE)\n","    \n","    return names_train, names_val, names_test,\\\n","            desc_train, desc_val, desc_test,\\\n","            keywords_train, keywords_val, keywords_test,\\\n","            X_train, X_val, X_test,\\\n","            y_train, y_val, y_test"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:24:37.747372Z","iopub.status.busy":"2023-06-26T15:24:37.747019Z","iopub.status.idle":"2023-06-26T15:24:38.251481Z","shell.execute_reply":"2023-06-26T15:24:38.250234Z","shell.execute_reply.started":"2023-06-26T15:24:37.747343Z"},"trusted":true},"outputs":[],"source":["USE_SAVED_INPUTS=True\n","if not USE_SAVED_INPUTS:\n","    names_train, names_val, names_test,\\\n","            desc_train, desc_val, desc_test,\\\n","            keywords_train, keywords_val, keywords_test,\\\n","            X_train, X_val, X_test,\\\n","            y_train, y_val, y_test = preprocess_inputs(df_development)\n","    \n","    names_train = torch.tensor(names_train, dtype=torch.int32)\n","    names_val = torch.tensor(names_val, dtype=torch.int32)\n","    names_test = torch.tensor(names_test, dtype=torch.int32)\n","    desc_train = torch.tensor(desc_train, dtype=torch.int32)\n","    desc_val = torch.tensor(desc_val, dtype=torch.int32)\n","    desc_test = torch.tensor(desc_test, dtype=torch.int32)\n","    keywords_train = torch.tensor(keywords_train, dtype=torch.int32)\n","    keywords_val = torch.tensor(keywords_val, dtype=torch.int32)\n","    keywords_test = torch.tensor(keywords_test, dtype=torch.int32)\n","    X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n","    X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n","    X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n","    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n","    y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n","    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n","    \n","    torch.save(names_train, '/kaggle/working/names_train.pt')\n","    torch.save(names_val, '/kaggle/working/names_val.pt')\n","    torch.save(names_test, '/kaggle/working/names_test.pt')\n","    torch.save(desc_train, '/kaggle/working/desc_train.pt')\n","    torch.save(desc_val, '/kaggle/working/desc_val.pt')\n","    torch.save(desc_test, '/kaggle/working/desc_test.pt')\n","    torch.save(keywords_train, '/kaggle/working/keywords_train.pt')\n","    torch.save(keywords_val, '/kaggle/working/keywords_val.pt')\n","    torch.save(keywords_test, '/kaggle/working/keywords_test.pt')\n","    torch.save(X_train, '/kaggle/working/X_train.pt')\n","    torch.save(X_val, '/kaggle/working/X_val.pt')\n","    torch.save(X_test, '/kaggle/working/X_test.pt')\n","    torch.save(y_train, '/kaggle/working/y_train.pt')\n","    torch.save(y_val, '/kaggle/working/y_val.pt')\n","    torch.save(y_test, '/kaggle/working/y_test.pt')\n","else:\n","    names_train = torch.load('/kaggle/input/kickstarter/names_train.pt')\n","    names_val = torch.load('/kaggle/input/kickstarter/names_val.pt')\n","    names_test = torch.load('/kaggle/input/kickstarter/names_test.pt')\n","    desc_train = torch.load('/kaggle/input/kickstarter/desc_train.pt')\n","    desc_val = torch.load('/kaggle/input/kickstarter/desc_val.pt')\n","    desc_test = torch.load('/kaggle/input/kickstarter/desc_test.pt')\n","    keywords_train = torch.load('/kaggle/input/kickstarter/keywords_train.pt')\n","    keywords_val = torch.load('/kaggle/input/kickstarter/keywords_val.pt')\n","    keywords_test = torch.load('/kaggle/input/kickstarter/keywords_test.pt')\n","    X_train = torch.load('/kaggle/input/kickstarter/X_train.pt')\n","    X_val = torch.load('/kaggle/input/kickstarter/X_val.pt')\n","    X_test = torch.load('/kaggle/input/kickstarter/X_test.pt')\n","    y_train = torch.load('/kaggle/input/kickstarter/y_train.pt')\n","    y_val = torch.load('/kaggle/input/kickstarter/y_val.pt')\n","    y_test = torch.load('/kaggle/input/kickstarter/y_test.pt')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:24:38.644668Z","iopub.status.busy":"2023-06-26T15:24:38.644036Z","iopub.status.idle":"2023-06-26T15:24:38.652654Z","shell.execute_reply":"2023-06-26T15:24:38.651459Z","shell.execute_reply.started":"2023-06-26T15:24:38.644631Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([86503, 22])\n","torch.Size([10813, 22])\n","torch.Size([10813, 22])\n","\n","torch.Size([86503, 46])\n","torch.Size([10813, 46])\n","torch.Size([10813, 46])\n","\n","torch.Size([86503, 21])\n","torch.Size([10813, 21])\n","torch.Size([10813, 21])\n","\n","torch.Size([86503, 30])\n","torch.Size([10813, 30])\n","torch.Size([10813, 30])\n","\n","torch.Size([86503])\n","torch.Size([10813])\n","torch.Size([10813])\n"]}],"source":["print(f'{names_train.shape}')\n","print(f'{names_val.shape}')\n","print(f'{names_test.shape}')\n","print()\n","print(f'{desc_train.shape}')\n","print(f'{desc_val.shape}')\n","print(f'{desc_test.shape}')\n","print()\n","print(f'{keywords_train.shape}')\n","print(f'{keywords_val.shape}')\n","print(f'{keywords_test.shape}')\n","print()\n","print(f'{X_train.shape}')\n","print(f'{X_val.shape}')\n","print(f'{X_test.shape}')\n","print()\n","print(f'{y_train.shape}')\n","print(f'{y_val.shape}')\n","print(f'{y_test.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["# Modelling"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:24:41.722281Z","iopub.status.busy":"2023-06-26T15:24:41.721907Z","iopub.status.idle":"2023-06-26T15:24:41.735999Z","shell.execute_reply":"2023-06-26T15:24:41.734629Z","shell.execute_reply.started":"2023-06-26T15:24:41.722251Z"},"trusted":true},"outputs":[],"source":["class MyModel(nn.Module):\n","    def __init__(self, num_features, name_vocab_size, name_embed_dim, desc_vocab_size, desc_embed_dim, keyword_vocab_size, keyword_embed_dim, num_classes):\n","        super(MyModel, self).__init__()\n","\n","        self.X_dense1 = nn.Linear(num_features, 32)\n","        self.X_dense2 = nn.Linear(32, 64)\n","        \n","        self.name_embedding = nn.Embedding(name_vocab_size, name_embed_dim)\n","        self.name_flatten = nn.Flatten()\n","\n","        self.desc_embedding = nn.Embedding(desc_vocab_size, desc_embed_dim)\n","        self.desc_flatten = nn.Flatten()\n","\n","        self.keyword_embedding = nn.Embedding(keyword_vocab_size, keyword_embed_dim)\n","        self.keyword_flatten = nn.Flatten()\n","\n","        self.concat = nn.Linear(5760, 128)\n","        self.outputs = nn.Linear(128, num_classes)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, X_inputs, name_inputs, desc_inputs, keyword_inputs):\n","#         print(f'{X_inputs.shape=}') # batch_size=32, num_features=30\n","\n","        X_dense1_out = torch.relu(self.X_dense1(X_inputs))\n","#         print(f'{X_dense1_out.shape=}') # batch_size=32, 32\n","        \n","        X_dense2_out = torch.relu(self.X_dense2(X_dense1_out))\n","#         print(f'{X_dense2_out.shape=}') # batch_size=32, 64\n","        \n","#         print(f'{name_inputs.shape=}') # batch_size=32, 22\n","        name_embed_out = self.name_embedding(name_inputs) # batch_size=32, max_seq_len_names=22, embed_dim_name=64\n","        name_flatten_out = self.name_flatten(name_embed_out) # batch_size=32, max_seq_len_names*embed_dim_name=22*64=1408\n","\n","#         print(f'{desc_inputs.shape=}') # batch_size=32, 46\n","        desc_embed_out = self.desc_embedding(desc_inputs)\n","#         print(f'{desc_embed_out.shape=}') # batch_size=32, max_seq_len_desc=46, embed_dim_desc=64\n","        desc_flatten_out = self.desc_flatten(desc_embed_out)\n","#         print(f'{desc_flatten_out.shape=}') # batch_size=32, max_seq_len_desc*embed_dim_desc=46*64=2944\n","\n","#         print(f'{keyword_inputs.shape=}') # batch_size=32, 21\n","        keyword_embed_out = self.keyword_embedding(keyword_inputs)\n","#         print(f'{keyword_embed_out.shape=}') # batch_size=32, max_seq_len_keywd=21, embed_dim_keywd=64\n","        keyword_flatten_out = self.keyword_flatten(keyword_embed_out)\n","#         print(f'{keyword_flatten_out.shape=}') # batch_size=32, max_seq_len_keywd*embed_dim_keywd=21*64=1344\n","\n","        concat_input = torch.cat((X_dense2_out, name_flatten_out, desc_flatten_out, keyword_flatten_out), dim=1)\n","        concat_out = torch.relu(self.concat(concat_input))   \n","        outputs = self.outputs(concat_out)\n","        outputs = self.sigmoid(outputs)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T15:24:42.126289Z","iopub.status.busy":"2023-06-26T15:24:42.125897Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1564f58059742b9b492c2796086e3f0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/100 | Train Loss: 0.7177 | Val Loss: 0.6808 | F1 Score: 0.5535\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3d45409c6304c7e966b1f5d66ff9181","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/100 | Train Loss: 0.6749 | Val Loss: 0.6687 | F1 Score: 0.5535\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9536ec23a0384ccdbdaa15731f49b480","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3/100 | Train Loss: 0.6642 | Val Loss: 0.6588 | F1 Score: 0.5535\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecb7c1653faa4ded9a3a368bc66be2ec","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["max_seq_length = X_train.shape[1]\n","name_vocab_size = 53376\n","name_embed_dim = 128\n","desc_vocab_size = 62559\n","desc_embed_dim = 128\n","keyword_vocab_size = 55293\n","keyword_embed_dim = 128\n","\n","num_classes = 1\n","learning_rate = 0.001\n","num_epochs = 100\n","batch_size = 1024\n","\n","train_dataset = TensorDataset(X_train, names_train, desc_train, keywords_train, y_train)\n","val_dataset = TensorDataset(X_val, names_val, desc_val, keywords_val, y_val)\n","test_dataset = TensorDataset(X_test, names_test, desc_test, keywords_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","model = MyModel(\n","    max_seq_length,\n","    name_vocab_size,\n","    name_embed_dim,\n","    desc_vocab_size,\n","    desc_embed_dim,\n","    keyword_vocab_size,\n","    keyword_embed_dim,\n","    num_classes\n",")\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","model.to(device)\n","\n","for epoch in range(1, num_epochs + 1):\n","    model.train()\n","    train_loss = 0.0\n","    pbar = tqdm(train_loader, total=len(train_loader))\n","    for X_inputs, names_inputs, desc_inputs, keywords_inputs, labels in pbar:\n","        X_inputs = X_inputs.to(device)\n","        names_inputs = names_inputs.to(device)\n","        desc_inputs = desc_inputs.to(device)\n","        keywords_inputs = keywords_inputs.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        predictions = model(X_inputs, names_inputs, desc_inputs, keywords_inputs)\n","        loss = criterion(predictions.squeeze(), labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        pbar.set_description(f'Training loss: {loss.item()=}')\n","        \n","    model.eval()\n","    val_loss = 0.0\n","    val_predictions = []\n","    val_targets = []\n","    with torch.no_grad():\n","        for X_inputs, names_inputs, desc_inputs, keywords_inputs, labels in val_loader:\n","            X_inputs = X_inputs.to(device)\n","            names_inputs = names_inputs.to(device)\n","            desc_inputs = desc_inputs.to(device)\n","            keywords_inputs = keywords_inputs.to(device)\n","            labels = labels.to(device)\n","\n","            predictions = model(X_inputs, names_inputs, desc_inputs, keywords_inputs)\n","            loss = criterion(predictions.squeeze(), labels)\n","\n","            val_loss += loss.item()\n","            val_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","            val_targets.extend(labels.cpu().numpy())\n","\n","    train_loss /= len(train_loader)\n","    val_loss /= len(val_loader)\n","    f1 = f1_score(val_targets, val_predictions, average='weighted')\n","\n","    # Print epoch statistics\n","    print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | F1 Score: {f1:.4f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
